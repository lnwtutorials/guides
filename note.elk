******************************************************************************
# wget files.grouplens.org/datasets/movielens/ml-latest-small.zip
# https://grouplens.org/datasets/movielens/
# http://media.sundog-soft.com/es7/MoviesToJson.py
# http://media.sundog-soft.com/es7/IndexRatings.py
# http://media.sundog-soft.com/es7/IndexTags.py
# http://media.sundog-soft.com/es/sayt.txt
******************************************************************************
-----------------------------------------------
==> [4. "Elasticsearch Overview" ]

The Elastic Stack

=> Elasticsearch
	- Started off as scalable Lucene
	- Horzontally scalable search engine
	- Each "shard" is an inverted index of documents
	- But not just for full text search!
	- Can handle structured data, and canaggregate data quickly
	- Often a faster solution than Hadoop/Spark/Flink/etc.
	- curl -XGET "http://127.0.0.1/tags/_search?pretty"

=> Kibana
	- Web UI for searching and visualizing
	- Complex aggregations, graphs, charts
	- Often used for log analysis
	
=> Logstash/Beats
	- Ways to feed data into elasticsearch
	- FileBeat can monitor log files, parse them, and import into elasticsearch in near-real-time
	- Logstash also pushes data into elasticsearch from many machines
	- Not just log files

=> X-Pack
	- Security
	- Altering
	- Monitoring
	- Reporting
	- Machine Learning
	- Graph Exploration
-----------------------------------------------
==> [5. Intro to Http and RESTful API's]

Rest: A Quick Intro

=> Anatomy of A Http Request
	- METHOD: 	The "verb" of the request. GET, POST, PUT, or DELETE
	- PROTOCOL:	What flavour of HTTP( HTTP/1.1)
	- HOST:		What web server you want to talk to
	- URL:		What resource is being requested
	- BODY:		Extra data needed by the server
	- HEADERS:	User-agent, content-type, etc.

=> Restful Api's
	- Pragmatic Definition: Using HTTP request to communicate with web services
	Example:
		GET requests retrieve information (like search results)
		PUT requests insert or replace new information
		DELETE requests delete information

=> Rest Fancy-speak
	- Representational State Transfer
	- Six guiding constraints:
		Client-server architecture
		Statelessness
		Cacheability
		Layered system
		Code on demand (ie, sending Javascript)
		Uniform interface

=> Why Rest ?
	- Language and system independent

=> The curl Command
	- A way to issue HTTP requests from the command line
	- From code, we'll use whatever library you use for HTTP/REST in the the same way.
	- curl -H "Content-Type: application/json" <URL> -d '<Body>'
	
-----------------------------------------------
# vim /etc/elasticsearch/elasticsearch.yml
node.name: node-1
network.host: 0.0.0.0
discovery.seed_hosts: ["127.0.0.1"]
cluster.initial_master_nodes: ["node-1"]

# curl -XGET 127.0.0.1:9200
# wget http://media.sundog-soft.com/es7/shakes-mapping.json
# cat shakes-mapping.json
{
        "mappings" : {
                "properties" : {
                        "speaker" : {"type": "keyword" },
                        "play_name" : {"type": "keyword" },
                        "line_id" : { "type" : "integer" },
                        "speech_number" : { "type" : "integer" }
                }
        }
}
# curl -H "Content-Type: application/json" -XPUT 127.0.0.1:9200/shakespeare --data-binary @shakes-mapping.json

# wget http://media.sundog-soft.com/es7/shakespeare_7.0.json
# curl -H "Content-Type: application/json" -XPOST '127.0.0.1:9200/shakespeare/_bulk' --data-binary @shakespeare_7.0.json
# curl -H "Content-Type: application/json" -XGET '127.0.0.1:9200/shakespeare/_search?pretty' -d '{ "query" : { "match_phrase":{"text_entry":"to be or not to be"}}}'


[https://www.elastic.co/guide/en/elasticsearch/reference/current/deb.html]

echo "keystore_password" > /path/to/my_pwd_file.tmp
chmod 600 /path/to/my_pwd_file.tmp
sudo systemctl set-environment ES_KEYSTORE_PASSPHRASE_FILE=/path/to/my_pwd_file.tmp
sudo systemctl start elasticsearch.service

curl -XGET 'http://localhost:9200/ilebear-*/_search?pretty'
-----------------------------------------------
GET _search 
{
"query" : {
	"match": {
		"firstname":  "Rowena"
		}
	}
}
-----------------------------------------------
GET _search 
{
"query" : {
	"range": {
		"balance": {
		"gte": 44999,
		"lte": 9000000
			}
		}
	}
}
-----------------------------------------------
POST _analyze
{
"analyzer" : "whitespace",
"text": "Elasticsearch is the heart of elastic stack"
}
-----------------------------------------------

A mapping is a "Schema definition". Elasticsearch has reasonable defaults, but sometimes you need to customize them.
curl -XPUT 127.0.0.1:9200/movies -d ' { "mappings" : { "properties" : { "year" : {"type" : "date" } }}}'

=> Common Mappings
Field types : String, byte,short,integer,long,float,double,boolean,date
* "properties" : {"user_id":{"type": "long"}}

Field Index : Do you want this field indexed for full-text search? analyzed/not_analyzed/no
* "properties" : {"genre":{"index":"not_analyzed"}}

Field Analyzer : Define your tokenizer and token filter. standard/whitespace/simple/english etc.
* "properties" : {"description":{"analyzer":"english"}}

==> More About Analyzers
- Character Filters : Remove HTML encoding, convert & to and
- Tokenizer : Split strings on whitespace/punctuation/non-letters
- Token Filter : Lowercasing, stemming, synonyms, stopwords

==> Choices for Analyzers
- Standard : Splits on word boundaries, remove punctuation lowercases. good choice if language is unknown
- Simple: Split on anything that isn't a letter, and lowercase
- Whitespace: Splits on whitespace but doesn't lowercase
- Language (i.e. english) : Accounts for language-specific stopwords and stemming

anon@host211:~$ mkdir bin
anon@host211:~$ vim bin/curl
anon@host211:~$ chmod a+x bin/*
anon@host211:~$ cd ~
anon@host211:~$ source .profile
anon@host211:~$ which curl
/home/anon/bin/ curl 

18. Import a Single Movie via JSON/REST
==> Import a Single Movie Via JSON
# curl -XPUT 127.0.0.1:9200/movies -d '{ "mappings": { "properties": {"year": {"type":"date"}}}}'
# curl -XGET 127.0.0.1:9200/_mappings 
# curl -XPUT 127.0.0.1:9200/movies/_doc/109487 -d '{"genre" : ["IMAX","Sci-Fi"], "title": "Interstellar", "year" : 2014 }'
# curl -XGET 127.0.0.1:9200/movies/_search?pretty

19. Insert Many Movies at once with the bulk API
==> Import a Bulk Movie Via JSON
# wget http://media.sundog-soft.com/es7/movies.json
# cat movies.json
{ "create" : { "_index" : "movies", "_id" : "135569" } }
{ "id": "135569", "title" : "Star Trek Beyond", "year":2016 , "genre":["Action", "Adventure", "Sci-Fi"] }
{ "create" : { "_index" : "movies", "_id" : "122886" } }
{ "id": "122886", "title" : "Star Wars: Episode VII - The Force Awakens", "year":2015 , "genre":["Action", "Adventure", "Fantasy", "Sci-Fi", "IMAX"] }
{ "create" : { "_index" : "movies", "_id" : "109487" } }
{ "id": "109487", "title" : "Interstellar", "year":2014 , "genre":["Sci-Fi", "IMAX"] }
{ "create" : { "_index" : "movies", "_id" : "58559" } }
{ "id": "58559", "title" : "Dark Knight, The", "year":2008 , "genre":["Action", "Crime", "Drama", "IMAX"] }
{ "create" : { "_index" : "movies", "_id" : "1924" } }
{ "id": "1924", "title" : "Plan 9 from Outer Space", "year":1959 , "genre":["Horror", "Sci-Fi"] }

# curl -XPUT 127.0.0.1:9200/_bulk?pretty --data-binary @movies.json

==> Updating data in elasticsearch ( immutable, but when updating version number incremented)
# curl -XPOST 127.0.0.1:9200/movies/_doc/109487/_update -d '{"doc":{"title":"Interstellar"}}'

==>[21] Deleting data in elasticsearch
# curl -XGET 127.0.0.1:9200/movies/_search?q=Dark
(found id=58559)
# curl -XDELETE 127.0.0.1:9200/movies/_doc/58559?pretty

==> [23] Deleaing with Concurrency. 
- Optimistic concurrency control (use retry_on _conflicts = N to automatically retry.)

# curl -XGET 127.0.0.1:9200/movies/_doc/109487?pretty

# curl -XPUT "127.0.0.1:9200/movies/_doc/109487?if_seq_no=7&if_primary_term=1" -d '{"genere" :["IMAX","Sci-Fi"], "title":"Interstellar foo", "year": 2014}'

#  curl -XPOST "127.0.0.1:9200/movies/_doc/109487/_update?retry_on_conflict=8" -d '{"doc":{"title":"Interstellar typo"}}'


==> Using Analyzer & Tokenizer
https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html
[Using Analyzer]
- Somethimes text fields should be exact match
	* Use Keyword mapping instead of text
- Search on analyzed text fields will return anything remotely relevant
	* Depending onAnd KeywordAnalyzer again does nothing. So, KeywordAnalyzer is used for things like ID or phone numbers, but not for usual text.
 the analyzer, result will be cas-insensitive, stemmed, stopwords removed, synonyms applied, etc.
	* Searches with multiple terms need not match them all

https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html
=> [Text Analyzer]
	- "Text analysis" is the process of converting unstructured text, like the body of an email or a product description, into a structured format that’s optimized for search.
	- If your index doesn’t contain text fields, no further setup is needed; you can skip the pages in this section.
	- if you use text fields or your text searches aren’t returning results as expected, configuring text analysis can often help
	- Should look into analysis configuration if you’re using Elasticsearch to :
		* Build a search engine
		* Mine unstructured data
		* Fine-tune search for a specific language
		* Perform lexicographic or linguistic research

=> [Quick View]
	- https://stackoverflow.com/questions/5483903/comparison-of-lucene-analyzers
	- https://www.elastic.co/guide/en/elasticsearch/reference/7.15/analysis-custom-analyzer.html
	- An analyzer is used at index Time and at search Time. It's used to create an index of terms.
	- To index a phrase, it could be useful to break it in words. Here comes the analyzer.
	- It applies tokenizers and token filters. A tokenizer could be a Whitespace tokenizer. It split a phrase in tokens at each space. A lowercase tokenizer will split a phrase at each non-letter and lowercase all letters.
	- A token filter is used to filter or convert some tokens. For example, a ASCII folding filter will convert characters like ê, é, è to e.
	- By default, Elasticsearch applies the standard analyzer. It will remove all common english words
	- An analyzer is a mix of all of that.

	- [Tokenizer] splits your text into chunks, and since different analyzers may use different tokenizers, you can get different output token streams, i.e. sequences of chunks of text. For example, KeywordAnalyzer you mentioned doesn't split the text at all and takes all the field as a single token. At the same time, StandardAnalyzer (and most other analyzers) use spaces and punctuation as a split points. For example, for phrase "I am very happy" it will produce list ["i", "am", "very", "happy"] (or something like that)
	
	- "Stemmers" are used to get the base of a word in question. It heavily depends on the language used. For example, for previous phrase in English there will be something like ["i", "be", "veri", "happi"] produced, and for French "Je suis très heureux" some kind of French analyzer (like SnowballAnalyzer, initialized with "French") will produce ["je", "être","tre", "heur"]. Of course, if you will use analyzer of one language to stem text in another, rules from the other language will be used and stemmer may produce incorrect results. It isn't fail of all the system, but search results then may be less accurate
	- "KeywordAnalyzer" doesn't use any stemmers, it passes all the field unmodified. So, if you are going to search some words in English text, it isn't a good idea to use this analyzer.
	
	- "Stop words" are the most frequent and almost useless words. Again, it heavily depends on language. For English these words are "a", "the", "I", "be", "have", etc. Stop-words filters remove them from the token stream to lower noise in search results, so finally our phrase "I'm very happy" with StandardAnalyzer will be transformed to list ["veri", "happi"].
	- And KeywordAnalyzer again does nothing. So, KeywordAnalyzer is used for things like ID or phone numbers, but not for usual text.
	
	- "StandAnalyzer" and "SmartCNAnalyzer". As I have to search text in Chinese. Obviously, SmartCnAnalyzer is better at handling Chinese. For diiferent purposes, you have to choose properest analyzer.
	
# curl -XGET 127.0.0.1:9200/movies/_search?pretty -d '{"query":{"match":{"title":"Star Trek"}}}'
# curl -XGET 127.0.0.1:9200/movies/_search?pretty -d '{"query":{"match_phrase":{"genre":"sci"}}}'

# curl -XDELETE 127.0.0.1:9200/movies/

curl -XPUT 127.0.0.1:9200/movies -d '{ 
"mappings":{
"properties": {
"id": {"type":"integer"},
"year": {"type":"date"},
"genre" : {"type":"keyword"},
"title" : {"type":"text","analyzer":"english"}
}}}'

# curl -XPUT 127.0.0.1:9200/movies -d '{"mappings":{"properties": {"id": {"type":"integer"},"year": {"type":"date"},"genre" : {"type":"keyword"},"title" : {"type":"text","analyzer":"english"}}}}'

==> Data Modeling & Parent/Child Relationship

curl -XPUT 127.0.0.1:9200/series -d '{
"mappings": {
"properties":{
"film_to_franchise": {
"type":"join",
"relations":{"franchise":"film"}
} } } }'

# curl -XPUT 127.0.0.1:9200/series -d ' {"mappings":{"properties":{"film_to_franchise":{"type":"join","relations":{"franchise":"film"}}}}}'

# wget http://media.sundog-soft.com/es7/series.json
# curl -XPUT 127.0.0.1:9200/_bulk?pretty --data-binary @series.json

[has_parent]
# curl -XGET 127.0.0.1:9200/series/_search?pretty -d '{"query":{"has_parent":{"parent_type":"franchise","query":{"match":{"title":"Star Wars"}}}}}'

[has_child]
# curl -XGET 127.0.0.1:9200/series/_search?pretty -d '{"query": {"has_child": {"type" : "film","query":{"match" : {"title":"The Force Awakens"}}}}}'


==> 27.Flattened Data Type
	- https://coralogix.com/blog/flattened-datatype-mappings-elasticsearch-tutorial/
	- https://www.elastic.co/guide/en/elasticsearch/reference/master/flattened.html
http://media.sundog-soft.com/es/flattened.txt
wget files.grouplens.org/datasets/movielens/ml-latest-small.zip
http://media.sundog-soft.com/es7/MoviesToJson.py
http://media.sundog-soft.com/es7/IndexRatings.py
http://media.sundog-soft.com/es7/IndexTags.py


# vim /etc/elasticsearch/elasticsearch.yml
node.name: node-1
network.host: 0.0.0.0
discovery.seed_hosts: ["127.0.0.1"]
cluster.initial_master_nodes: ["node-1"]

# curl -XGET 127.0.0.1:9200
# wget http://media.sundog-soft.com/es7/shakes-mapping.json
# curl -H "Content-Type: application/json" -XPUT 127.0.0.1:9200/shakespeare --data-binary @shakes-mapping.json

# wget http://media.sundog-soft.com/es7/shakespeare_7.0.json
# curl -H "Content-Type: application/json" -XPOST '127.0.0.1:9200/shakespeare/_bulk' --data-binary @shakespeare_7.0.json
# curl -H "Content-Type: application/json" -XGET '127.0.0.1:9200/shakespeare/_search?pretty' -d '{ "query" : { "match_phrase":{"text_entry":"to be or not to be"}}}'


[https://www.elastic.co/guide/en/elasticsearch/reference/current/deb.html]

echo "keystore_password" > /path/to/my_pwd_file.tmp
chmod 600 /path/to/my_pwd_file.tmp
sudo systemctl set-environment ES_KEYSTORE_PASSPHRASE_FILE=/path/to/my_pwd_file.tmp
sudo systemctl start elasticsearch.service

curl -XGET 'http://localhost:9200/ilebear-*'/_search?pretty'
-----------------------------------------------
GET _search 
{
"query" : {
	"match": {
		"firstname":  "Rowena"
		}
	}
}
-----------------------------------------------
GET _search 
{
"query" : {
	"range": {
		"balance": {
		"gte": 44999,
		"lte": 9000000
			}
		}
	}
}
-----------------------------------------------
POST _analyze
{
"analyzer" : "whitespace",
"text": "Elasticsearch is the heart of elastic stack"
}
-----------------------------------------------

A mapping is a "Schema definition". Elasticsearch has reasonable defaults, but sometimes you need to customize them.
curl -XPUT 127.0.0.1:9200/movies -d ' { "mappings" : { "properties" : { "year" : {"type" : "date" } }}}'

=> Common Mappings
Field types : String, byte,short,integer,long,float,double,boolean,date
* "properties" : {"user_id":{"type": "long"}}

Field Index : Do you want this field indexed for full-text search? analyzed/not_analyzed/no
* "properties" : {"genre":{"index":"not_analyzed"}}

Field Analyzer : Define your tokenizer and token filter. standard/whitespace/simple/english etc.
* "properties" : {"description":{"analyzer":"english"}}

==> More About Analyzers
- Character Filters : Remove HTML encoding, convert & to and
- Tokenizer : Split strings on whitespace/punctuation/non-letters
- Token Filter : Lowercasing, stemming, synonyms, stopwords

==> Choices for Analyzers
- Standard : Splits on word boundaries, remove punctuation lowercases. good choice if language is unknown

- Simple: Split on anything that isn't a letter, and lowercase

- Whitespace: Splits on whitespace but doesn't lowercase

- Language (i.e. english) : Accounts for language-specific stopwords and stemming

anon@host211:~$ mkdir bin
anon@host211:~$ vim bin/curl
anon@host211:~$ chmod a+x bin/*
anon@host211:~$ cd ~
anon@host211:~$ source .profile
anon@host211:~$ which curl
/home/anon/bin/ curl 

==> Import a Single Movie Via JSON
# curl -XPUT 127.0.0.1:9200/movies -d '{ "mappings": { "properties": {"year": {"type":"date"}}}}'
# curl -XGET 127.0.0.1:9200/_mappings 
# curl -XPUT 127.0.0.1:9200/movies/_doc/109487 -d '{"genre" : ["IMAX","Sci-Fi"], "title": "Interstellar", "year" : 2014 }'
# curl -XGET 127.0.0.1:9200/movies/_search?pretty

==> Import a Bulk Movie Via JSON

{ "create" : { "_index" : "movies", "_id" : "135569" } }
{ "id": "135569", "title" : "Star Trek Beyond", "year":2016 , "genre":["Action", "Adventure", "Sci-Fi"] }
{ "create" : { "_index" : "movies", "_id" : "122886" } }
{ "id": "122886", "title" : "Star Wars: Episode VII - The Force Awakens", "year":2015 , "genre":["Action", "Adventure", "Fantasy", "Sci-Fi", "IMAX"] }
{ "create" : { "_index" : "movies", "_id" : "109487" } }
{ "id": "109487", "title" : "Interstellar", "year":2014 , "genre":["Sci-Fi", "IMAX"] }
{ "create" : { "_index" : "movies", "_id" : "58559" } }
{ "id": "58559", "title" : "Dark Knight, The", "year":2008 , "genre":["Action", "Crime", "Drama", "IMAX"] }
{ "create" : { "_index" : "movies", "_id" : "1924" } }
{ "id": "1924", "title" : "Plan 9 from Outer Space", "year":1959 , "genre":["Horror", "Sci-Fi"] }

# curl -XPUT 127.0.0.1:9200/_bulk?pretty --data-binary @movies.json

==> Updating data in elasticsearch ( immutable, but when updating version number incremented)
# curl -XPOST 127.0.0.1:9200/movies/_doc/109487/_update -d '{"doc":{"title":"Interstellar"}}'

==> Deleting data in elasticsearch
# curl -XGET 127.0.0.1:9200/movies/_search?q=Dark
(found id=58559)
# curl -XDELETE 127.0.0.1:9200/movies/_doc/58559?pretty

==> Deleaing with Concurrency. 
- Optimistic concurrency control (use retry_on _conflicts = N to automatically retry.)

# curl -XGET 127.0.0.1:9200/movies/_doc/109487?pretty

# curl -XPUT "127.0.0.1:9200/movies/_doc/109487?if_seq_no=7&if_primary_term=1" -d '{"genere" :["IMAX","Sci-Fi"], "title":"Interstellar foo", "year": 2014}'

#  curl -XPOST "127.0.0.1:9200/movies/_doc/109487/_update?retry_on_conflict=8" -d '{"doc":{"title":"Interstellar typo"}}'


==> Using Analyzer & Tokenizer

[Using Analyzer]
- Somethimes text fields should be exact match
	* Use Keyword mapping instead of text
- Search on analyzed text fields will return anything remotely relevant
	* Depending on the analyzer, result will be cas-insensitive, stemmed, stopwords removed, synonyms applied, etc.
	* Searches with multiple terms need not match them all

# curl -XGET 127.0.0.1:9200/movies/_search?pretty -d '{"query":{"match":{"title":"Star Trek"}}}'
# curl -XGET 127.0.0.1:9200/movies/_search?pretty -d '{"query":{"match_phrase":{"genre":"sci"}}}'

# curl -XDELETE 127.0.0.1:9200/movies/

curl -XPUT 127.0.0.1:9200/movies -d '{
"mappings":{
"properties": {
"id": {"type":"integer"},
"year": {"type":"date"},
"genre" : {"type":"keyword"},
"title" : {"type":"text","analyzer":"english"}
}
}
}'

# curl -XPUT 127.0.0.1:9200/movies -d '{"mappings":{"properties": {"id": {"type":"integer"},"year": {"type":"date"},"genre" : {"type":"keyword"},"title" : {"type":"text","analyzer":"english"}}}}'

==>[26] Data Modeling & Parent/Child Relationship

curl -XPUT 127.0.0.1:9200/series -d '{
"mappings": {
"properties":{
"film_to_franchise": {
"type":"join",
"relations":{"franchise":"film"}
} } } }'

# curl -XPUT 127.0.0.1:9200/series -d ' {"mappings":{"properties":{"film_to_franchise":{"type":"join","relations":{"franchise":"film"}}}}}'

# wget http://media.sundog-soft.com/es7/series.json
# curl -XPUT 127.0.0.1:9200/_bulk?pretty --data-binary @series.json

[has_parent]
# curl -XGET 127.0.0.1:9200/series/_search?pretty -d '{"query":{"has_parent":{"parent_type":"franchise","query":{"match":{"title":"Star Wars"}}}}}'

[has_child]
# curl -XGET 127.0.0.1:9200/series/_search?pretty -d '{"query": {"has_child": {"type" : "film","query":{"match" : {"title":"The Force Awakens"}}}}}'


==> Flattened Data Type
# curl -XPUT "http://127.0.0.1:9200/demo-default/_doc/1" -d'{
"message": "[5592:1:0309/123054.737712:ERROR:child_process_sandbox_support_impl_linux.cc(79)] FontService unique font name matching request did not receive a response."
"fileset": { "name" : "syslog"},
"process": { "name" : "org.gnome.shell.desktop", "pid": 3381 },
"@timestamp": "2020-03-09T18:00:54.000+05:30",
"host": {"hostname":"bionic","name":"bionic"}
}'


# curl -XGET "http://127.0.0.1:9200/demo-default/_mapping?pretty=true"
# curl -XGET "http://127.0.0.1:9200/_cluster/state?pretty=true" >> es-cluster-state.json

[Updating Cluster State]
......................................................................
Node-2	<====Cluster(State)======== Node-1 ======Cluster(State) ======> Node-3
Node-2	====(Ack From Node-2)=====> Node-1 <=====(Ack From Node-3)===== Node-3
......................................................................
# curl -XPUT "http://127.0.0.1:9200/demo-flattened"

# curl -XPUT "http://127.0.0.1:9200/demo-flattened/_mapping" -d'{
"properties" : { "host" : {"type": "flattened"} }
}'

# curl -XGET "http://127.0.0.1:9200/demo-flattened/_mapping?pretty=true"

# curl -XPOST "http://127.0.0.1:9200/demo-flattened/_update/1" -d'{
"doc": {
"host": {"osVersion":"Bionic Beaver", "osArchitecture":"x86_64"}
}}'

# curl -XGET "http://127.0.0.1:9200/demo-flattened/_doc/1?pretty=true"
# curl -XGET "http://127.0.0.1:9200/demo-flattened/_mapping?pretty=true"

# curl -XGET "http://127.0.0.1:9200/demo-flattened/_search?pretty=true" -d'{"query":{"match":{"host":"Bionic Beaver"}}}'

 * [Supported Queries for flattened Data Types]
	- term, terms and terms_set
	- prefix
	- range (non numerical range operations)
	- match and multi_match ( we have to supply exact Keywords)
	- query_string and simple_query_string
	- exists
http://media.sundog-soft.com/es/flattened.txt

---------------------------------------------

==> [28. Dealing with Mapping Exception]
[Mapping]
* Process : Defining how JSON documents will be stored
* Result : The actual metadata resulting from the definion process

1. Explicit Mapping
	- Fields and their type are predefined.
2. Dynamic Mapping
	- Fields and theri types are automatically defined by Elasticsearch.

[The Mapping Result]
"Timestamp" mapped as a date
"Service" mapped as a keyword
"IP" mapped as an ip datatypes
"port" mapped as an integer
"Message" mapped as text 

{"mappings" : { "properties" :
"timetamp" : {"type":"date"},
"service" : {"type" : "keyword" },
"host_ip" : {"type" : "ip" },
"port" : {"type" : "integer" },
"message" : {"type" : "text" }
}}

[Mapping Challenges]
=> Explicit Mapping
	- Mapping exceptions when there's a mismatch
=> Dynamic Mapping
	- May lead to a mapping explosion

http://media.sundog-soft.com/es/exceptions.txt

1.
curl --request PUT 'http://localhost:9200/microservice-logs' \
--data-raw '{
   "mappings": {
       "properties": {
           "timestamp": { "type": "date"  },
           "service": { "type": "keyword" },
           "host_ip": { "type": "ip" },
           "port": { "type": "integer" },
           "message": { "type": "text" }
       }
   }
}'


2.
{"timestamp": "2020-04-11T12:34:56.789Z", "service": "ABC", "host_ip": "10.0.2.15", "port": 12345, "message": "Started!" }

3.

curl --request POST 'http://localhost:9200/microservice-logs/_doc?pretty' \
--data-raw '{"timestamp": "2020-04-11T12:34:56.789Z", "service": "XYZ", "host_ip": "10.0.2.15", "port": "15000", "message": "Hello!" }'

4.
curl --request POST 'http://localhost:9200/microservice-logs/_doc?pretty' \
--data-raw '{"timestamp": "2020-04-11T12:34:56.789Z", "service": "XYZ", "host_ip": "10.0.2.15", "port": "NONE", "message": "I am not well!" }'

5.
curl --request POST 'http://localhost:9200/microservice-logs/_close'
 
curl --location --request PUT 'http://localhost:9200/microservice-logs/_settings' \
--data-raw '{
   "index.mapping.ignore_malformed": true
}'
 
curl --request POST 'http://localhost:9200/microservice-logs/_open'

6.
curl --request POST 'http://localhost:9200/microservice-logs/_doc?pretty' \
--data-raw '{"timestamp": "2020-04-11T12:34:56.789Z", "service": "XYZ", "host_ip": "10.0.2.15", "port": "NONE", "message": "I am not well!" }'

7.
curl --request POST 'http://localhost:9200/microservice-logs/_doc?pretty' \
--data-raw '{"timestamp": "2020-04-11T12:34:56.789Z", "service": "ABC", "host_ip": "10.0.2.15", "port": 12345, "message": {"data": {"received":"here"}}}'

8.
curl --request POST 'http://localhost:9200/microservice-logs/_doc?pretty' \
--data-raw '{"timestamp": "2020-04-11T12:34:56.789Z", "service": "ABC", "host_ip": "10.0.2.15", "port": 12345, "message": "Received...", "payload": {"data": {"received":"here"}}}'

9.
curl --request POST 'http://localhost:9200/microservice-logs/_doc?pretty' \
--data-raw '{"timestamp": "2020-04-11T12:34:56.789Z", "service": "ABC", "host_ip": "10.0.2.15", "port": 12345, "message": "Received...", "payload": {"data": {"received": {"even": "more"}}}}'

10.
thousandone_fields_json=$(echo {1..1001..1} | jq -Rn '( input | split(" ") ) as $nums | $nums[] | . as $key | [{key:($key|tostring),value:($key|tonumber)}] | from_entries' | jq -cs 'add')
 echo "$thousandone_fields_json"
-----------------------------------------------

# apt install jq -y

# curl --location --request PUT 'https://localhost:9200/big-objects'

# curl --request POST 'http://localhost:9200/big-objects/_doc?pretty' --data-raw "$thousandone_fields_json"

# curl --location --request POST 'http://localhost:9200/big-objects/_settings' \
 --data-raw '{"index.mapping.total_fields.limit":1001 }'

-----------------------------------------------
==> [31. "query lite" Interface]

/movies/_search?q=title:star
/movies/_search?q=+year:>2010+title:trek

And It can be Dangerous
- Cryptic and tough to debug
- Can be a security issue if exposed to end users
- Fragile : one wrong character and you're hosted
	*But it's handy for quick experimenting
# curl -XGET 127.0.0.1:9200/movies/_search?q=title:star&pretty
# curl -XGET 127.0.0.1:9200/movies/_search?q=+year:>2010+title:trek&pretty

URI Search
-----------------------------------------------
==> [32. "Json Search In-Depth"] 

* Request Body Search
- Query DSL in the request body as JSON (Yes, a GET request can have a body!)
curl -XGET 127.0.0.1:9200/movies/_search?pretty -d '{"query":{"match":{"title":"star"}}}'

* Queries and Filters
- Filters ask a yes/no question of your data
- Queries return data in term of relevance

Use filters when you can - they are faster and cacheable.

[Example: Boolean query with a filter]
====================================== (must mean and query)
curl -XGET 127.0.0.1:9200/movies/_search?pretty -d '{"query":{"bool":{
"must":{"term":{"title":"trek"}},
"filter":{"range":{"year":{"gte":2010}}}
}}}'

[Some types of filter]
======================================
- Term: Filter by exact values
{"term":{"year":2014}}

- Term: Match if any exact values in a list match
{"term":{"genre":["Sci-Fi","Adventure"] }}

- Range: Find numbers or dates in a given range (gt, gte, lt, lte)
{"range":{"year": {"gte": 2010}}}

- Exists: Find documents where a field exists
{"exists": {"field":"tags"}}

- Missing
{"missing": {"field":"tags"}}

-Bool : Combine filters with Boolean logic( must, must_not, should)


[Some types of Queries]
======================================
- Match_all: Return all documents and is the default. Normally used with a filter.
{"match_all":{}}
- Match: Searches analyzed results, such as full texts search.
	{"match":{"title":"star"}}
- Multi_match: Run the same query on multiple fields.
	{"multi_match":{"query":"star","fields":["title","synopsis"]}}
- Bool: Works like a bool filter, but results are scored by relevance.

[Syntax Reminder]
======================================
Queries are wrapped in a "query" : { } block,
Filters are wrapped in a "filter" : { } block.

You can combine filters inside queries, or queries inside filters too

curl -XGET 127.0.0.1:9200/movies/_search?pretty -d '{ "query": "bool" : { "must": {"term":{"title":"trek"}}, "filter":{"range":{"year":{"gte":2010}}} }}}'

-----------------------------------------------
==> [33. "Phrase Matching"]

[Phrase Search]
- Must find all term, in the right order.
curl -XGET 127.0.0.1:9200/movies/_search?pretty -d'{"query":{"match_phrase":{"title":"star wars" }}}'

[Slop]
- Order matters,but you're OK with some words being in between the terms:
	curl -XGET 127.0.0.1:9200/movies/_search?pretty -d'{"query":{"match_phrase":{"title":{"query":"star beyond", "slop":1} }}}'
- The "slop" represents how far you're willing to let a term move to satisfy a phrase)in either direction!)
Another example:"quick brown fox" would match "quick fox" with a slop of 1.

[Proximity Queries]
- Remember this is a query - result are sorted by relevance.
- Just use a really hight slop if you wan to get any documents that contains the words in phrase, but want documents that have the words closer together scored higher.

	curl -XGET 127.0.0.1:9200/movies/_search?pretty -d'{"query":{"match_phrase":{"title":{"query":"star beyond", "slop":100} }}}'
-----------------------------------------------
==> [34. "Querying Different Way"]
Q. Search for "Star Wars" movies released after 1980, using both a "URI search" and a "request body search."

curl -XGET "127.0.0.1:9200/movies/_search?q=+year:>1980+title:star%20wars&pretty"

curl -XGET 127.0.0.1:9200/movies/_search -d'{"query":{
"bool": {
"must":{"match_phrase":{"title":"Star Wars"}},
"filter":{"range":{"year":{"gte":1980}}}
}}}'

-----------------------------------------------
==> [35. "Pagination"]

- Specify "From" And "Size"
- Pagination Syntax

curl -XGET '127.0.0.1:9200/movies/_search?size=2&from=2&pretty'

curl -XGET 127.0.0.1:9200/movies/_search -d'{ "from": 2, "size":2, "query": {"match":{"genre":"Sci-Fi"}}}'

[Beware]
-----------
- Deep pagination can kill performance.
- Every result must be retrieved, collected and sorted.
- Enforce an upper bound on how many result you'll return to users.

-----------------------------------------------
==> [36. "Sorting"]

- Sorting your result is usually Quite simple
	curl -XGET '127.0.0.1:9200/movies/_search?sort=year&pretty'

=> Unless you're Dealing with Strings.
- A string field that is "analyzed" for full-text search can't be used to sort documents
- This is because it exists in the inverted index as individual term, not as the entire string.

=> If need to Sort on an Analyzed Text field, Map A Keyword Copy
# curl -H "Content-Type:application/json" -XPUT "127.0.0.1:9200/movies" -d'{
> "mappings" :{
> "properties" :{
> "title" : {
> "type" : "text",
> "fields" : {
> "raw" : {
> "type" : "keyword"
> }}}}}}'

=> Now you can sort on the keyword "Raw" Field.
curl -XGET '127.0.0.1:9200/movies/_search?sort=title.raw&pretty'

- Sadly, you cannot change the mapping on an existing index.
- You'd have to delete it, set up a new mapping, and re-index it.
- Like the number of shards, this is something you should think about before importing data into your index.


# curl -XDELETE '127.0.0.1:9200/movies'
# curl -XPUT 127.0.0.1:9200/movies -d '{"mappings":{"properties": {"title": {"type":"text","fields":{"raw":{"type":"keyword"}}}}}}'
# curl -XPUT '127.0.0.1:9200/_bulk --data-binary @movies.json'
# curl -XGET '127.0.0.1:9200/movies/_search?sort=year'
# curl -XGET '127.0.0.1:9200/movies/_search?sort=title.raw&pretty'

-----------------------------------------------
==> [37. "More with Filters"]
=> Another Filtered Query
curl -XGET 127.0.0.1:9200/movies/_search -d'{
"query":{
	"bool": {
		"must":{"match":{"genre":"Sci-Fi"}},
		"must_not":{"match":{"title":"trek"}},
		"filter":{"range":{"year":{"gte":2010,"lt":2015 }}}
}}}'

-----------------------------------------------
==> [38. "Excercise"]
Q. Search for science fiction movies before 1960, sorted by title.

Ans.

curl -XGET 127.0.0.1:9200/movies/_search?sort=title.raw&pretty -d'{
"query" : {
"bool" : {
"must" : {"match" : {"title" : "Sci-Fi"}},
"filter":{"range": {"year":{"lt":1960 }}}
}}}'

-----------------------------------------------
==> [39. "Fuzzy Queries"]

=> Fuzzy Matches :: A way to account for typos and misspellings

The "levenshtein edit distance" accounts for:
- Substitutions of characters(interstellar -> intersteller)
- Insertions of characters(interstellar -> insterstellar)
- Deletion of characters (interstellar -> interstelar)

All of avobe have an edit distance of 1.


=> The Fuzziness Parameter
curl -XGET 127.0.0.1:9200/movies/_search?pretty -d'{
"query": {
	"fuzzy" : {
		"title" : {"value" : "intrsteller","fuzziness":2}
}}}'

=> Auto Fuziness
	Fuziness:AUTO
- 0 for 1-2 character strings
- 1 for 3-5 character strings
- 2 for anything else

curl -XGET 127.0.0.1:9200/movies/_search?pretty -d'{ "query" : {"match":{"title":"intersteller" }}}'
curl -XGET 127.0.0.1:9200/movies/_search?pretty -d'{ "query" : {"fuzzy":{"title":{"value":"intersteller","fuzziness":2}}}}'
curl -XGET 127.0.0.1:9200/movies/_search?pretty -d'{ "query" : {"fuzzy":{"title":{"value":"warz","fuzziness":1}}}}'
-----------------------------------------------
==> [40. "Partial Matching"]

=> Prefix Queries On Strings
- If we remapped year to be a string...
curl -XGET '127.0.0.1:9200/movies/_search?pretty' -d '
{ 
"query" : {
	"prefix":{
		"year":"201"}}
}'

- Wildcard Queries
curl -XGET '127.0.0.1:9200/movies/_search?pretty' -d '
{ 
"query" : {
	"wildcard":{
		"year":"1*"}}
}'
		------------(regexp queries also exist)

# curl -XDELETE 127.0.0.1:9200/movies
# curl -XPUT 127.0.0.1:9200/movies -d'{"mappings":{"properties":{"year":{"type":"text"}}}}'
# curl -XPUT 127.0.0.1:9200/_bulk --data-binary @movies.json

# curl -XGET 127.0.0.1:9200/movies/_search?pretty -d'{"query":{"prefix":{"year":"201"}}}'
# curl -XGET 127.0.0.1:9200/movies/_search?pretty -d'{"query":{"wildcard":{"year":"19*"}}}'

-----------------------------------------------
==> [41. "Query-time Search As You Type"]
- Abusing sloppiness...
curl -XGET '127.0.0.1:9200/movies/_search?pretty' -d '{
"query": { 
	"match_phrase_prefix": {
	"title" :{
		"query" :" star trek",
		"slop": 10
}}}}'

-----------------------------------------------
==> [42. "N-Grams Part-1"]

=> Index-time With N-grams

====="star":=============
unigram: 	[s,t,a,r]
bigram: 	[st,ta,ar]
trigram:	[sta,tar]
4-gram:		[star]
	*** Edge N-grams are built only on the beginning of each term
=>[Indexing N-grams]
- Create an "Autocomplete" analyzer
curl -XPUT '127.0.0.1:9200/movies?pretty' -d '{
"settings": {
"analysis": {
"filter" : {
	"autocomplete_filter": {
	"type":"edge_ngram",
	"min_gram":1,
	"max_gram":20 }
	},
"analyzer": {
	"autocomplete" : {
	"type" : "custom",
	"tokenizer": "standard",
	"filter": ["lowercase","autocomplete_filter"]
	}
}}}'

=>[Now Map Our Field With It]
curl -XPUT '127.0.0.1:9200/movies/_mapping?pretty' -d '{
"query": {
	"match": {
	"title" : { "query" : "sta",
	"analyzer" : "standard"
		}}
	}
}'

	*** Otherwise our query will also get split into n-grams, and we'll get results for everthing that matches 's','t','a','st',etc.


=>[But Only Use N-grams On The Index Side!]
curl -XGET '127.0.0.1:9200/movies/_search?pretty' -d '{
"properties" : {
"title" : { "type" :"text" , "analyzer" : "autocomplete" }
	}
}'

=>[Completion Suggesters]
- You can also upload a list of all possible completions ahead of time using completion suggesters.
-----------------------------------------------
==> [43. "N-Grams Part-2"] Excercise

# curl -XPUT '127.0.0.1:9200/movies?pretty' -d '{
"settings": {
"analysis": {
"filter" : {
"autocomplete_filter": {
	"type":"edge_ngram",
	"min_gram":1,
	"max_gram":20 
	}},
"analyzer": {
"autocomplete" : {
"type" : "custom",
"tokenizer": "standard",
"filter": ["lowercase","autocomplete_filter"]
}}}}}'

# curl -XGET 127.0.0.1:9200/movies/_analyze?pretty -d '{"analyzer":"autocomplete","text":"Sta"}'
# curl -XPUT '127.0.0.1:9200/movies/_mapping?pretty' -d '{ "properties" : { "title": { "type":"text", "analyzer":"autocomplete" }}}'
# curl -XPUT '127.0.0.1:9200/_bulk' --data-binary @movies.json
# curl -XGET '127.0.0.1:9200/movies/_search?pretty' -d '{ "query": {"match": {"title" : "sta" }}}'
# curl -XGET '127.0.0.1:9200/movies/_search?pretty' -d '{ "query": { "match": { "title" : { "query" : "sta", "analyzer" : "standard" }}}}'
# curl -XGET '127.0.0.1:9200/movies/_search?pretty' -d '{
"query": {
	"match": {
	"title" : { "query" : "Star tre",
	"analyzer" : "standard"
		}}
	}
}'
-----------------------------------------------
==> [44. "Search As you Type" Field Type]
=> Autocomplete with Search_as_you_type
=> Type-as-you-search Datatype

(movie_title) ["Star Wars: Episode VII- The Force Awakens"]
	|
	|->----mapped as a datatype---------------------> (subfield)["s","st","sta","star"]
		search_as_you_type

#http://media.sundog-soft.com/es/sayt.txt
1.
# curl --silent --request POST 'http://localhost:9200/movies/_analyze?pretty' --data-raw '{ "tokenizer" : "standard", "filter": [{"type":"edge_ngram", "min_gram": 1, "max_gram": 4}], "text" : "Star" }'

2.
# curl --request PUT 'http://localhost:9200/autocomplete' -d '{ "mappings": { "properties": { "title": { "type": "search_as_you_type" }, "genre": { "type": "search_as_you_type" } } } }'

3.
# curl --silent --request POST 'http://localhost:9200/_reindex?pretty' --data-raw '{ "source": { "index": "movies" }, "dest": { "index": "autocomplete" } }' | grep "total\|created\|failures"

4.
# curl -s --request GET 'http://localhost:9200/autocomplete/_search?pretty' --data-raw '{ "size": 5, "query": { "multi_match": { "query": "Sta", "type": "bool_prefix", "fields": ["title","title._2gram","title._3gram"] }}}'

5.
# while true 
do
 IFS= read -rsn1 char
 INPUT=$INPUT$char
 echo $INPUT
 curl --silent --request GET 'http://localhost:9200/autocomplete/_search' \
 --data-raw '{
     "size": 5,
     "query": {
         "multi_match": {
             "query": "'"$INPUT"'",
             "type": "bool_prefix",
             "fields": [
                 "title",
                 "title._2gram",
                 "title._3gram"
             ]
         }
     }
 }' | jq .hits.hits[]._source.title | grep -i "$INPUT"
done

-----------------------------------------------
==> [47. "Importing Data With a Script"]
=> You can Import From Just About Anything
- Stand-alone "scirpts" can submit bulk documents via REST API
- "Logstash" and "beats" can stream data from logs, S3, databases, and more
- AWS systems can stream in data via "lambda" or "kinesis firehose"
- "Kafka", "spark" and more have Elasticsearch integration add-ons

=> Hack Together A Script
- Read in data from some distributed filesystem
- Transform it into JSON bulk inserts
- Submit via HTTP/REST to your elasticsearch cluster

import csv
import re
csvfile = open('ml-latest-small/movies.csv', 'r')
reader = csv.DictReader( csvfile )
for movie in reader:
        print ("{ \"create\" : { \"_index\": \"movies\", \"_id\" : \"" , movie['movieId'], "\" } }", sep='')
        title = re.sub(" \(.*\)$", "", re.sub('"','', movie['title']))
        year = movie['title'][-5:-1]
        if (not year.isdigit()):
            year = "2016"
        genres = movie['genres'].split('|')
        print ("{ \"id\": \"", movie['movieId'], "\", \"title\": \"", title, "\", \"year\":", year, ", \"genre\":[", end='', sep='')
        for genre in genres[:-1]:
            print("\"", genre, "\",", end='', sep='')
        print("\"", genres[-1], "\"", end = '', sep='')
        print ("] }")

# http://media.sundog-soft.com/es7/MoviesToJson.py

wget files.grouplens.org/datasets/movielens/ml-latest-small.zip
wget http://media.sundog-soft.com/es7/MoviesToJson.py
python3 MoviesToJson.py > moremovies.json
curl -XDELETE 127.0.0.1:9200/movies
curl -XPUT 127.0.0.1:9200/_bulk --data-binary @moremovies.json
curl -XGET 127.0.0.1:9200/movies/_search?q=marry%20poppins&
-----------------------------------------------
==> [48. "Importing with Client Linraries" ]

A Less Hacky Script.
Free elasticsearch client libraries are available for pretty much any language.
- "Java" has cleint maintained by elastic.co
- "Python" has an elasticsearch package
- Elasticsearch-"ruby"
- Serveral choices for "scala"
- Elasticsearch.pm module for "perl"

you don't have to wrangle JSON
es = elasticsearch.Elasticsearch()
es.indices.delete(index="ratings",ignore=404)
deque(helper.parallel_bulk(es,readRatings(),index="ratings"),maxlen=0)
es.indices.refresh()


=> For completeness:
----------------------------------------------
import csv
from collections import deque
import elasticsearch
from elasticsearch import helpers

def readMovies():
	csv = open('ml-latest-small/movies.csv','r')
	reader = csv.DictReader( csvfile )
	titleLookup = {}
	for movie in reader:
		titleLookup[movie['movieId']] = movie['title']
	return titleLookup

def readRatings():
	csv = open('ml-latest-small/ratings.csv','r')
	titleLookup = readMovies()
	reader = csv.DictReader( csvfile )

	for line in reader:
	rating = {}
	rating['user_id'] = int(line['userId'])
	rating['movie_id'] = int(line['movie_id'])
	rating['title'] = int(line['movieId'])
	rating['rating'] = int(line['rating'])
	rating['timestamp'] = int(line['timestamp'])
	yield rating

	es = elasticsearch.Elasticsearch()
	es.indices.delete(index="ratings",ignore=404
	deque(helpers.parallel_bulk(es,radRatings(),index="ratings"),maxlen=0)
	es.indices.refresh()

# apt install python3-pip
# pip3 install elasticsearch
# wget http://media.sundog-soft.com/es7/IndexRatings.py
# python3 IndexRatings.py
# curl -XGET 127.0.0.1:9200/ratings/_search?pretty

-----------------------------------------------
==> [49. ] Importing with a script (Excercise)
Write a script to import the tags.csv data from ml-latest-small into a new "tags" index.

Solution :-

import csv
from collections import deque
import elasticsearch
from elasticsearch import helpers

def readMovies():
    csvfile = open('ml-latest-small/movies.csv', 'r')
    reader = csv.DictReader( csvfile )
    titleLookup = {}
    for movie in reader:
            titleLookup[movie['movieId']] = movie['title']
    return titleLookup

def readTags():
    csvfile = open('ml-latest-small/tags.csv', 'r')
    titleLookup = readMovies()
    reader = csv.DictReader( csvfile )
    for line in reader:
        tag = {}
        tag['user_id'] = int(line['userId'])
        tag['movie_id'] = int(line['movieId'])
        tag['title'] = titleLookup[line['movieId']]
        tag['tag'] = line['tag']
        tag['timestamp'] = int(line['timestamp'])
        yield tag

es = elasticsearch.Elasticsearch()
es.indices.delete(index="tags",ignore=404)
deque(helpers.parallel_bulk(es,readTags(),index="tags"), maxlen=0)
es.indices.refresh()

# wget http://media.sundog-soft.com/es7/IndexTags.py
# curl -XGET 127.0.0.1:9200/tags/_search?pretty

-----------------------------------------------
50.[Logstash]

==> What Logstash for ::
(( Files, S3, Beats, Kafka )) ===> Logstash ===> (( Elasticsearch,AWS,Hadoop,Mongodb ))

==> It's More than Plumbing
- Logstash "parses", "transforms", and filters data as it passes through.
- It can "derive structure" from unstructured data
- It can "anonymize" personal data or exclude it entirely.
- It can do "geo-location" lookups
- It can scale across many nodes
- It guarantees at-least-once delivery
- It absorbs thoroughput from load spikes
https://www.elastic.co/guide/en/logstash/current/filter-plugins.html

==>Huge Variety of Input Source Events
Elastic Beats - Cloudwatch - Couchdb - Drupal - Elasticsearch - Windows Event Log - Shell Output - Local Files - Ganglia - Gelf -Gemfire - Random - Generator - Github - Google Pubsub - Graphite - Hearbeats - Heroku - Http - Imap - IRC -JDBC - JMX - Kafka - Lumberjack - Meetup - Command Pipes - Puppet - Rabbitmq - Rackspace Cloud Queue - Redis - Relp -Rss - S3 - Salesforce - Snmp -Sqlite -Sqs -Stdin -Stomp - Syslog - Tcp - Twitter - Udp - Unix Sockets - Varnish Log - Websocket -Wmi -Xmpp - Zenoss - Zeromq

==>Huge Variety of Output "Stash" destination
boundary - circonus - cloudwatch -csv - datadoghq - elasticsearch - email -exec - local file - ganglia -gelf -bigquery - google cloud storage - graphite - graphitastic - pipchat - http - influxdb - irc -jira - juggernaut - kafka - librato - loggly - lumberjack - meticcatcher - mongodb - nagios - new relic insights - opentsdb - pagerduty - pipe to stdin - rabbitmq - rackspace cloud queue - redis -redmine - riak - reimann - s3 -sns - solr -sqs - statsd - stdout - stomp - syslog - tcp - udp - webhdfs - websocket - xmpp - zabbix - zeromq


[Typcal Usage]
(Files or Beats) Web Logs -----> (Logstash) Parse into Structured Fields, Geolocate ---------> Elasticsearch


-----------------------------------------------
==> [51. "Installing Logstash" ]

[Configuring Logstash]
# vim /etc/logstash/conf.d/logstash.conf
input {file {
path => "/home/student/access_log"
start_position => "beginning"
}}

filter {
        grok { match => {"message" => "%{COMBINEDAPACHELOG}" } }
        date { match => ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"] }
}

output {
        elasticsearch { hosts => ["localhost:9200"] }
        stdout {codec => rubydebug }
}

# wget media.sundog-soft.com/es/access_log
# sudo apt install openjdk-8-jre-headless
# sudo apt update && sudo apt install logstash
# sudo vim /etc/logstash/conf.d/logstash.conf

https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html
-----------------------------------------------
==> [52. "Running Logstash" ]

# wget http://media.sundog-soft.com/es/access_log
# cd /usr/share/logstash
# sudo bin/logstash -f /etc/logstash/conf.d/logstash.conf
	or
# sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/logstash.conf

# curl -XGET 127.0.0.1:9200/_cat/indices?v
# curl -XGET 127.0.0.1:9200/logstash-2019.05.07-000001/_search?pretty


-----------------------------------------------
==> [53. "Logstash and Mysql, Part 1" ]

Logstash With MySql

Install A Jdbc Driver
Get a platform-independent mysql connector form
	https://dev.mysql.com/downloads/connector/j/
# wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.15.zip
# Unzip mysql-connector-java-8.0.15.zip


==> Configure Logstash
input { 
    jdbc {
	jdbc_connection_String => "jdbc:mysql://localhost:3306/movielens"
	jdbc_user => "anon"
	jdbc_password => "password"
	jdbc_driver_library => "/home/anon/mysql-connector-java-8.0.15/mysql-connector-java-8.0.15.jar"
	jdbc_driver_class => "com.mysql.jdbc.Driver"
	statement => "SELECT * FROM movies"
	}
}


# sudo apt install mysql-server
# wget http://files.grouplens.org/datasets/movielens/ml-100k.zip
# uzip ml-100k.zip
# sudo mysql -uroot -ppassword
# sudo mysql -u root -p
> create database movielens;
> create table movielens.movies (movieID INT primary Key Not Null, title Text, releaseDate date) CHARACTER SET utf8mb4;
> load data local infile 'ml-100k/u.item' into table movielens.movies
    -> CHARACTER SET utf8mb4
    -> fields terminated by '|'
    -> (movieID,title,@var3)
    -> set releaseDate = str_to_date(@var3, '%d-%M-%Y');
Note : 
	=> SHOW GLOBAL VARIABLES LIKE 'local_infile';
	=> SET GLOBAL local_infile=1; 					{1-On/enable, 0-off/disable}
> use movielens;
> select * from movies where title Like 'Star%';


-----------------------------------------------
==> [54. "Logstash and Mysql, Part 2" ]

https://dev.mysql.com/downloads/connector/j/

Platform independent -------> .zip archive download ----------> wget "URL"

#vim /etc/logstash/conf.d/mysql.conf
input {
	jdbc {
		jdbc_connection_String => "jdbc:mysql://localhost:3306/movielens"
		jdbc_user => "anon"
		jdbc_password => "password"
		jdbc_driver_library => "/home/anon/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar"
		jdbc_driver_class => "com.mysql,jdbc.Driver"
	}
}

output {
	stdout { codec => json_lines }
	elasticsearch { 5
		hosts => ["localhost:9200"]
		index => "movielens-sql"
		}
}

# mysql -uroot -p
> create user 'anon'@'localhost' identified by 'password' ;
> grant all privileges on *.* to 'anon'@'localhost' ;
> flush privileges ;

# cd /usr/share/logstash
# sudo bin/logstash -f /etc/logstash/conf.d/mysql.conf
# curl -XGET 'localhost:9200/movielens-sql/_search?q=title.Star&pretty'

-----------------------------------------------
==> [55. "Importing CSV Data With Logstash" ] //csv-read //csv-read-drop

Logstash: Import & Parse CSV

What is CSV data ?
	- Comma Separated Format
	- https://github.com/coralogix-resources/elk-course-samples 
	
# mkdir -p /home/anon/csv-data 
# cd /home/anon/csv-data
# curl -O https://raw.githubusercontent.com/coralogix-resources/elk-course-samples/master/csv-schema-short-numerical.csv ; cd -
# sudo wget -P /etc/logstash/conf.d/ https://raw.githubusercontent.com/coralogix-resources/elk-course-samples/master/csv-read.conf
# sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/csv-read.conf
# curl -XGET 'localhost:9200/demo-csv/_search?pretty=true

# sudo wget -P /etc/logstash/conf.d/ https://raw.githubusercontent.com/coralogix-resources/elk-course-samples/master/csv-read-drop.conf
# sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/csv-read-drop.conf
# curl -XGET localhost:9200/demo-csv-drop/_search?pretty=true
# curl -XGET localhost:9200/demo-csv-drop/_mapping/field/age?pretty=true
-----------------------------------------------
==> [56. "Importing JSON Data With Logstash" ]

Logstash: Import & Parse JSON
	- Java Script Object Notation
	- Key/value pair

=> JSON Basic concept	
	- Structured in Key/Value pairs and ordered lists
	- Keys and Values are separated by a colon ":"
	- Double quotes surround each key
	- Key/Value pairs are separated by a comma ","
	- JSON filetype extention is .json
	
=> JSON Data Types
	- Booleans: True or false values
		* {"member":true}
	- Strings: Unicode characters that are wrapped in double quotes.
		* {"name":"Emma"}
	- Numbers: Numeric character without quotes.
		* {"age":41}
	- Null: An empty value
		* {"lastName":null}
	- Arrays: Ordered list, wrapped in square brackets and can include other JSON Object.
		* {"customer":{"name":"Emma","member":"true"} }
	- JSON Objects: Written as a collection of key.value pairs and wrapped in curly brackets.
		* {"orderItems":["camera","light","lens"] }

=> 	Logstash JSON Filter
JSON-1, JSON-2, JSON-3 ---->> Logstash -----(Parse & Filter)------>> Elasticsearch

# mkdir /home/anon/json-data ; cd /home/anon/json-data
# wget https://media.sundog-soft.com/es/sample-json.log

# cd /etc/logstash/conf.d/
# sudo vim json-read.conf
input {
	file {
		start_position => "beginning"
		path => "/home/anon/json-data/sample-json.log"
		sincedb_path => "/dev/null"
		}
}
filter {
	json {
		source => "message"
	}
}
output {
	elasticsearch { 
		hosts => "http://localhost:9200"
		index => "demo-json"
		}
	stdout {}
}


# sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/json-read.conf
# curl localhost:9200/demo-json/_search?pretty=true
# curl -XGET http://127.0.0.1:9200/_cat/indices/demo-json?v

# cp /etc/logstash/conf.d/json-read.conf /etc/logstash/conf.d/json-drop.conf
# vim /etc/logstash/conf.d/json-drop.conf
input {
	file {
		start_position => "beginning"
		path => "/home/anon/json-data/sample-json.log"
		sincedb_path => "/dev/null"
		}
}
filter {
	json {
		source => "message"
	}
	if [paymentType] == "Mastercard" 
	{
		drop {}
	}
	mutate {
		remove_field => ["message", "@timestamp", "path", "host", "@version"]
	}
}
output {
	elasticsearch { 
		hosts => "http://localhost:9200"
		index => "demo-json-drop"
		}
	stdout {}
}

# sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/json-drop.conf
# curl localhost:9200/demo-json-drop/_search?pretty=true

# http://127.0.0.1:9200/demo-csv-drop/_search?q=name:Rod&pretty
# cd json-data
# wget https://media.sundog-soft.com/es/sample-json-split.log
# vim /etc/logstash/conf.d/json-split.conf
input {
	file {
		# type => "json"
		start_position => "beginning"
		path => "/home/anon/json-data/sample-json-split.log"
	}
}
filter {
	json 	{ source => "message"	}
	split 	{ field => "["pastEvents"]"	}
	mutate	{ remove_field => ["message","@timestamp","path","host","@version"]}
}

output {
	elasticsearch {
		hosts => "http://localhost:9200"
		index => "demo-json-split"
	}
	stdout {}
}

# sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/json-split.conf
# curl "http://localhost:9200/demo-json-split/_search?pretty=true"


# cp /etc/logstash/conf.d/json-split.conf /etc/logstash/conf.d/json-split-structured.conf
# vim /etc/logstash/conf.d/json-split-structured.conf
input {
	file {
		# type => "json"
		start_position => "beginning"
		path => "/home/anon/json-data/sample-json-split.log"
	}
}
filter {
	json 	{ source => "message"	}
	split 	{ field => "["pastEvents"]"	}
	mutate	{ remove_field => ["message","@timestamp","path","host","@version"]}
	
	add_field => {
		"eventId" => "%{[pastEvents][eventId]}"
		"transactionId" => "%{[pastEvents][transactionId]}"
		}
	remove_field => ["message", "@timestamp", "path", "host", "@verson","pastEvents"] 
	}
}
output {
	elasticsearch {
		hosts => "http://localhost:9200"
		index => "demo-json-split-structured"
		}
	stdout {}
	}


# sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/json-split-structured.conf
# curl -XGET "http://localhost:9200/demo-json-split-/_search?pretty=true"

-----------------------------------------------
==> [57. "Logstash and S3"]

What is S3
	- Amazon Web Service's Simple Storage Service.
	- Cloud-based distributed storage system

Integration Is Easy-peasy.
input { 
	s3{
		bucket => "sundog-es"
		access_key_id => "ABCD****XY**Z"
		secret_access_key =>
		}
	}
filter {}
output {}

-----------------------------------------------
==> [58 . "Parsing and Filtering Logstash with Grok" ]

=> Structured Data Example: CSV

=> UnStructured Data Example: Linux System Logs

=> Regular Expressions:
	- Regex Patter
		: ^([a-zA-Z0-9_\-\.]+)@([a-zA-Z0-0_\-\.]+)\.([a-zA-Z]{2,3}))$
	- Matches
		: john@example.com

=> Predefined Grok Patterns: https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html

=> Email Grok Pattern:
			- %{EMAILADDRESS:client_email}

Example:
	Pattern:
		- %{TIMESTAMP_ISO8601:time} %{LOGLEVEL:logvelvel} %{GREEDYDATA:logMessage}
	Match
		- 2020-07-16T19:20:21.45+01:00 DEBUG This is a sample log

Testing Grok Patterns 
		- Grok Debug Tool: https://grokdebug.herokuapp.com/

mkdir 03-grok-examples
wget http://media.sundog-soft.com/es/sample.log
wget http://media.sundog-soft.com/es/grok-example.conf
sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/grok-example.conf  

curl -XGET "http://localhost:9200/demo-grok/_search?pretty=true" -d'{"_source":["loglevel","time","logMessage"]}'
curl -XDELETE "http://localhost:9200/demo-grok"

wget http://media.sundog-soft.com/es/grok-example-02.conf
sudo mv grok-example-02.conf grok-example-02.conf
sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/grok-example.conf
curl -XGET "http://localhost:9200/demo-grok-multiple/_search?pretty=true" -d'{"_source":{"excludes":["@timestamp","host","path"]}}'
-----------------------------------------------
==> [59. "Logstash Grok Examples for Common Log Formats" ]

sudo git clone https://github.com/coralogix-resource/logstash.git /etc/logstash/conf.d/logstash
wget https://github.com/coralogix-resources/logstash/archive/refs/heads/master.zip

https://media.sundog-soft.com/es/grok.txt

==> NGINX ACCESS LOGS
1. https://raw.githubusercontent.com/coralogix-resources/logstash/master/nginx/access.log
2. https://grokdebug.herokuapp.com/

3. https://raw.githubusercontent.com/coralogix-resources/logstash/master/nginx/nginx-access-final.conf
input {
	file {
	   path => ["/etc/logstash/conf.d/logstash/nginx/access.log"]
	   start_position => "beginning"
	   sincedb_path => "/dev/null"
	 }
}
filter {
      grok {
        match => { "message" => ["%{IPORHOST:remote_ip} - %{DATA:user_name} \[%{HTTPDATE:access_time}\] \"%{WORD:http_method} %{DATA:url} HTTP/%{NUMBER:http_version}\" %{NUMBER:response_code} %{NUMBER:body_sent_bytes} \"%{DATA:referrer}\" \"%{DATA:agent}\""] }
        remove_field => "message"
      }
      mutate {
        add_field => { "read_timestamp" => "%{@timestamp}" }
      }
      date {
        match => [ "timestamp", "dd/MMM/YYYY:H:m:s Z" ]
        remove_field => "timestamp"
      }
}
output{
  elasticsearch{
    hosts => ["localhost:9200"] 
    index => "nginx-access-logs-02" 
  }
  stdout { 
    codec => "rubydebug"
   }
}
4. 
curl -XGET "http://localhost:9200/nginx-access-logs-02/_search?pretty" -d'{
  "size": 1, 
  "track_total_hits": true,
  "query": {
    "bool": {
      "must_not": [
        {
          "term": {
            "tags.keyword": "_grokparsefailure"
          }
        }
      ]
    }
  }
}'

==> IIS LOGS
5. https://raw.githubusercontent.com/coralogix-resources/logstash/master/iis/u_ex171118-sample.log

6. https://raw.githubusercontent.com/coralogix-resources/logstash/master/iis/iis-final-working.conf
input {
      file {
        path => ["/etc/logstash/conf.d/logstash/iis/u_ex171118-sample.log"]
        start_position => "beginning"
        sincedb_path => "/dev/null"
      }
    }

filter {
     grok {
      match => {
       "message" => ["%{TIMESTAMP_ISO8601:time} %{WORD:method} %{URIPATH:uri_requested} %{NOTSPACE:query} %{NUMBER:port} %{NOTSPACE:username} %{IPORHOST:client_ip} %{NOTSPACE:http_version} %{NOTSPACE:user_agent} %{NOTSPACE:cookie} %{URI:referrer_url} %{IPORHOST:host} %{NUMBER:http_status_code} %{NUMBER:protocol_substatus_code} %{NUMBER:win32_status} %{NUMBER:bytes_sent} %{NUMBER:bytes_received} %{NUMBER:time_taken}"
       ]
     }
   }

     mutate {
      rename => {
       "@timestamp" => "read_timestamp"
      }
     }
     date {
      match => [
       "time",
       "yyyy-MM-dd HH:mm:ss"
      ]
      target => "@timestamp"
     }

    mutate {
    remove_field => ["message", "host", "@version", "path"]
    }

   #if "_grokparsefailure" in [tags] { drop{} }
  
}

output {
  elasticsearch {
            hosts => [ "localhost:9200"]
            index => "iis-log"
        }
  stdout { codec => rubydebug }
}

7.
curl -XGET "http://localhost:9200/iis-log/_search?pretty" -d'{
  "size": 1, 
  "track_total_hits": true,
  "query": {
    "bool": {
      "must_not": [
        {
          "term": {
            "tags.keyword": "_grokparsefailure"
          }
        }
      ]
    }
  }
}'

==> MONGODB LOGS
8. https://raw.githubusercontent.com/coralogix-resources/logstash/master/mongodb/mongodb.log

9. https://raw.githubusercontent.com/coralogix-resources/logstash/master/mongodb/mongodb-final.conf
input {
file {
   path => ["/etc/logstash/conf.d/logstash/mongodb/mongodb.log"]
   start_position => "beginning"
   sincedb_path => "/dev/null"
 }
}
filter {
      grok {
        match => { "message" => ["%{TIMESTAMP_ISO8601:timestamp}\s+%{NOTSPACE:severity}\s+%{NOTSPACE:component}\s+(?:\[%{DATA:context}\])?\s+%{GREEDYDATA:log_message}" ] }
        remove_field => "message"
      }
      mutate {
        add_field => { "read_timestamp" => "%{@timestamp}" }
      }
}
output {
  elasticsearch {
            hosts => [ "localhost:9200"]
            index => "mongo-logs-01"
        }
  stdout { codec => rubydebug }
}

10. 
curl -XGET "http://localhost:9200/mongo-logs-01/_search?pretty" -d'{
  "size": 1, 
  "track_total_hits": true,
  "query": {
    "bool": {
      "must_not": [
        {
          "term": {
            "tags.keyword": "_grokparsefailure"
          }
        }
      ]
    }
  }
}'


==> USER AGENT MAPPING AND IP TO GEO LOCATION MAPPING IN LOGS
11. https://raw.githubusercontent.com/coralogix-resources/logstash/master/apache/access_log
12. https://raw.githubusercontent.com/coralogix-resources/logstash/master/apache/apache-access-enriched.conf
input {
file {
   path => ["/etc/logstash/conf.d/logstash/apache/access_log"]
   start_position => "beginning"
   sincedb_path => "/dev/null"
 }
}
filter {
      grok {
         match => { "message" => ["%{COMBINEDAPACHELOG}"] }
        remove_field => "message"
      }
      mutate {
        add_field => { "read_timestamp" => "%{@timestamp}" }
      }
      date {
        match => [ "timestamp", "dd/MMM/YYYY:H:m:s Z" ]
        remove_field => "timestamp"
      }
     useragent {
       source => "agent"
       target => "agent"
     }
     geoip {
       source => "clientip"
       target => "geoip"
     }
}
output {
  elasticsearch {
            hosts => [ "localhost:9200"]
            index => "apache-logs"
        }
  stdout { codec => rubydebug }
}

13. 
curl -XGET "http://localhost:9200/apache-logs/_search?pretty" -d'{
  "size": 1,
  "track_total_hits": true,
  "query": {
  "bool": {
    "must_not": [
      {
        "term": {
          "tags.keyword": "_grokparsefailure"
        }
      }
    ]
  }
  }
}'

==> ELASTICSEARCH LOGS
14. https://raw.githubusercontent.com/coralogix-resources/logstash/master/elasticsearch_logs/elasticsearch.log
15. https://raw.githubusercontent.com/coralogix-resources/logstash/master/elasticsearch_logs/es-logs-final.conf
input {
  file {
    path => "/etc/logstash/conf.d/logstash/elasticsearch_logs/elasticsearch.log"
    type => "elasticsearch"
    start_position => "beginning" 
    sincedb_path => "/dev/null"
    codec => multiline {
      pattern => "^\["
      negate => true
      what => "previous"
    }
  }
}

filter {
  if [type] == "elasticsearch" {
    grok {
      match => [ "message", "\[%{TIMESTAMP_ISO8601:timestamp}\]\[%{DATA:severity}%{SPACE}\]\[%{DATA:source}%{SPACE}\]%{SPACE}(?<message>(.|\r|\n)*)" ]
      overwrite => [ "message" ]
    }

    if "_grokparsefailure" not in [tags] {
      grok {  
        match => [
          "message", "^\[%{DATA:node}\] %{SPACE}\[%{DATA:index}\]%{SPACE}(?<short_message>(.|\r|\n)*)",
          "message", "^\[%{DATA:node}\]%{SPACE}(?<short_message>(.|\r|\n)*)" ]
        tag_on_failure => []
      }
    }
  }
}

output {
  elasticsearch {
            hosts => [ "localhost:9200"]
            index => "es-test-logs"
        }
  stdout { codec => rubydebug }
}

16.
curl -XGET "http://localhost:9200/es-test-logs/_search?pretty" -d'{
  "size": 1, 
  "query": {
    "bool": {
      "must_not": [
        {
          "match": {
            "tags": "multiline"
          }
        }
      ]
    }
  }
}'

ELASTICSEARCH SLOW LOGS
17. https://raw.githubusercontent.com/coralogix-resources/logstash/master/elasticsearch_slowlogs/es_slowlog.log

18. https://raw.githubusercontent.com/coralogix-resources/logstash/master/elasticsearch_slowlogs/es-slowlog-final.conf
# comment
input{
	file{
		path => ["/etc/logstash/conf.d/logstash/elasticsearch_slowlogs/es_slowlog.log"]
                start_position => "beginning"
                sincedb_path => "/dev/null"
		codec => plain {
                    charset => "ISO-8859-15" #Reads plaintext with no delimiting between events
        	}
	}
}
filter {
	grok {
		match => { "message" => ['\[%{TIMESTAMP_ISO8601:timestamp}\]\[%{LOGLEVEL:level}\]\[%{HOSTNAME:type}\]%{SPACE}\[%{HOSTNAME:[node_name]}\]%{SPACE}\[%{WORD:[index_name]}\]%{NOTSPACE}%{SPACE}took\[%{NUMBER:took_micro}%{NOTSPACE}\]%{NOTSPACE}%{SPACE}%{NOTSPACE}%{NOTSPACE}%{SPACE}%{NOTSPACE}%{NOTSPACE}%{SPACE}%{NOTSPACE}%{NOTSPACE}%{SPACE}search_type\[%{WORD:search_type}\]%{NOTSPACE}%{SPACE}total_shards\[%{NUMBER:total_shards}\]%{NOTSPACE}%{SPACE}source%{GREEDYDATA:query}\Z']}
	}
	
	mutate{
		remove_field => ["@version","@timestamp","host","path","logTook"] 
	}
} 
output{
	elasticsearch{
		hosts => ["localhost:9200"] 
		index => "es-slow-logs" 
	}
	stdout { 
		codec => "rubydebug"
	 }
}

19. curl -XGET "http://localhost:9200/es-slow-logs/_search?pretty" -d'{  "size": 1}'

MYSQL SLOW LOGS
20. https://raw.githubusercontent.com/coralogix-resources/logstash/master/mysql_slowlogs/mysql-slow.log

21. https://raw.githubusercontent.com/coralogix-resources/logstash/master/mysql_slowlogs/mysql-slowlogs.conf
input {
file {
   path => ["/etc/logstash/conf.d/logstash/mysql_slowlogs/mysql-slow.log"]
   start_position => "beginning"
   sincedb_path => "/dev/null"
   codec => multiline {
          pattern => "^# Time: %{TIMESTAMP_ISO8601}"
          negate => true
          what => "previous"
        }
 }
}
filter {
      mutate {
        gsub => [
          "message", "#", "",
          "message", "\n", " "
        ]
        remove_field => "host"
      }
      grok {
        match => { "message" => [
              "Time\:%{SPACE}%{TIMESTAMP_ISO8601:timestamp}%{SPACE}User\@Host\:%{SPACE}%{WORD:user}\[%{NOTSPACE}\] \@ %{NOTSPACE:host} \[\]%{SPACE}Id\:%{SPACE}%{NUMBER:sql_id}%{SPACE}Query_time\:%{SPACE}%{NUMBER:query_time}%{SPACE}Lock_time\:%{SPACE}%{NUMBER:lock_time}%{SPACE}Rows_sent\:%{SPACE}%{NUMBER:rows_sent}%{SPACE}Rows_examined\:%{SPACE}%{NUMBER:rows_examined}%{SPACE}%{GREEDYDATA}; %{GREEDYDATA:command}\;%{GREEDYDATA}" 
       ] }
      }
      
      mutate {
        add_field => { "read_timestamp" => "%{@timestamp}" }
        #remove_field => "message"
      }
}
output {
  elasticsearch {
            hosts => [ "localhost:9200"]
            index => "mysql-slowlogs-01"
        }
  stdout { codec => rubydebug }
}

22.
curl -XGET "http://localhost:9200/mysql-slowlogs-01/_search?pretty" -d'{
  "size":1,
  "query": {
    "bool": {
    "must_not": [
      {
        "term": {
          "tags.keyword": "_grokparsefailure"
        }
      }
    ]
  }
  }

}'

==> AWS ELASTIC LOAD BALANCER
23. https://raw.githubusercontent.com/coralogix-resources/logstash/master/aws_elb/elb_logs.log

24. https://raw.githubusercontent.com/coralogix-resources/logstash/master/aws_elb/aws-elb.conf
input {
file {
   path => ["/etc/logstash/conf.d/logstash/aws_elb/elb_logs.log"]
   start_position => "beginning"
   sincedb_path => "/dev/null"
 }
}
filter {
      grok {
        match => { "message" => ["%{TIMESTAMP_ISO8601:timestamp} %{NOTSPACE:loadbalancer} %{IP:client_ip}:%{NUMBER:client_port} (?:%{IP:backend_ip}:%{NUMBER:backend_port}|-) %{NUMBER:request_processing_time} %{NUMBER:backend_processing_time} %{NUMBER:response_processing_time} (?:%{NUMBER:elb_status_code}|-) (?:%{NUMBER:backend_status_code}|-) %{NUMBER:received_bytes} %{NUMBER:sent_bytes} \"(?:%{WORD:verb}|-) (?:%{GREEDYDATA:request}|-) (?:HTTP/%{NUMBER:httpversion}|-( )?)\" \"%{DATA:userAgent}\"( %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol})?"] }
        remove_field => "message"
      }
}
output {
#if "_grokparsefailure" in [tags] {
stdout { codec => rubydebug }
#}
elasticsearch {
            hosts => [ "localhost:9200"]
            index => "aws-elb-logs"
        }
}

25.
curl -XGET "http://localhost:9200/aws-elb-logs/_search?pretty" -d'
{
  "size": 1,
  "query": {
    "bool": {
      "must_not": [
        {
        "term": {
          "tags": {
            "value": "_grokparsefailure"
          }
        }
      }
      ]
    }
  }
}'

==> AWS APPLICATION LOAD BALANCER
26. https://raw.githubusercontent.com/coralogix-resources/logstash/master/aws_alb/alb_logs.log

27. https://raw.githubusercontent.com/coralogix-resources/logstash/master/aws_alb/aws-alb.conf
input {
file {
   path => ["/etc/logstash/conf.d/logstash/aws_alb/alb_logs.log"]
   start_position => "beginning"
   sincedb_path => "/dev/null"
 }
}
filter {
      grok {
        match => { "message" => ["%{NOTSPACE:request_type} %{TIMESTAMP_ISO8601:log_timestamp} %{NOTSPACE:alb-name} %{NOTSPACE:client}:%{NUMBER:client_port} (?:%{IP:backend_ip}:%{NUMBER:backend_port}|-) %{NUMBER:request_processing_time} %{NUMBER:backend_processing_time} %{NOTSPACE:response_processing_time:float} %{NOTSPACE:elb_status_code} %{NOTSPACE:target_status_code} %{NOTSPACE:received_bytes:float} %{NOTSPACE:sent_bytes:float} %{QUOTEDSTRING:request} %{QUOTEDSTRING:user_agent} %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} %{NOTSPACE:target_group_arn} %{QUOTEDSTRING:trace_id}"] }
        remove_field => "message"
      }
}
output {
#if "_grokparsefailure" in [tags] {
stdout { codec => rubydebug }
#}
elasticsearch {
            hosts => [ "localhost:9200"]
            index => "aws-alb-logs"
        }
}

28.
curl -XGET "http://localhost:9200/aws-alb-logs/_search?pretty" -d'
{
  "size": 1,
  "query": {
    "bool": {
      "must_not": [
        {"term": {
          "tags": {
            "value": "_grokparsefailure"
          }
        }
      }
      ]
    }
  }
}'

==> AWS CLOUDFRONT
29. https://raw.githubusercontent.com/coralogix-resources/logstash/master/aws_cloudfront/cloudfront_logs.log

30. https://raw.githubusercontent.com/coralogix-resources/logstash/master/aws_cloudfront/aws-cloudfront.conf
input {
file {
   path => ["/etc/logstash/conf.d/logstash/aws_cloudfront/cloudfront_logs.log"]
   start_position => "beginning"
   sincedb_path => "/dev/null"
 }
}

filter {
mutate {
        gsub => [
          "message", "\t", " ",
          "message", "\n", " "
        ]
      }
grok {
        match => { "message" => [
              "%{DATE:date}[ \t]%{TIME:time}[ \t]%{DATA:x_edge_location}[ \t](?:%{NUMBER:sc_bytes}|-)[ \t]%{IP:c_ip}[ \t]%{WORD:cs_method}[ \t]%{HOSTNAME:cs_host}[ \t]%{NOTSPACE:cs_uri_stem}[ \t]%{NUMBER:sc_status}[ \t]%{GREEDYDATA:referrer}[ \t]%{NOTSPACE:user_agent}[ \t]%{GREEDYDATA:cs_uri_query}[ \t]%{NOTSPACE:cookie}[ \t]%{WORD:x_edge_result_type}[ \t]%{NOTSPACE:x_edge_request_id}[ \t]%{HOSTNAME:x_host_header}[ \t]%{URIPROTO:cs_protocol}[ \t]%{INT:cs_bytes}[ \t]%{NUMBER:time_taken}[ \t]%{NOTSPACE:x_forwarded_for}[ \t]%{NOTSPACE:ssl_protocol}[ \t]%{NOTSPACE:ssl_cipher}[ \t]%{NOTSPACE:x_edge_response_result_type}[ \t]%{NOTSPACE:cs_protocol_version}[ \t]%{NOTSPACE:fle_status}[ \t]%{NOTSPACE:fle_encrypted_fields}[ \t]%{NOTSPACE:c_port}[ \t]%{NOTSPACE:time_to_first_byte}[ \t]%{NOTSPACE:x_edge_detailed_result_type}[ \t]%{NOTSPACE:sc_content_type}[ \t]%{NOTSPACE:sc_content_len}[ \t]%{NOTSPACE:sc_range_start}[ \t]%{NOTSPACE:sc_range_end}" 
       ] }
      }
mutate {
  remove_field => "message"
}
if "_grokparsefailure" in [tags] { drop {} }
}

output {
stdout { codec => rubydebug }
elasticsearch {
            hosts => [ "localhost:9200"]
            index => "aws-cloudfront-logs"
        }
}

31.
curl -XGET "http://localhost:9200/aws-cloudfront-logs/_search?pretty" -d'
{
  "query": {
    "bool": {
      "must_not": [
        {"term": {
          "tags": {
            "value": "_grokparsefailure"
          }
        }
      }
      ]
    }
  }
}'

CLEANUP
curl -XDELETE localhost:9200/nginx-access-logs-02
curl -XDELETE localhost:9200/iis-log
curl -XDELETE localhost:9200/mongo-logs-01
curl -XDELETE localhost:9200/apache-logs
curl -XDELETE localhost:9200/es-test-logs
curl -XDELETE localhost:9200/es-slow-logs
curl -XDELETE localhost:9200/mysql-slowlogs-01
curl -XDELETE localhost:9200/aws-elb-logs
curl -XDELETE localhost:9200/aws-alb-logs
curl -XDELETE localhost:9200/aws-cloudfront-logs

-----------------------------------------------
==> [60. "Logstash Input Plugins, Part-1: Hearbeat"]

https://github.com/coralogix-resources/logstash-input-plugins.git
wget https://github.com/coralogix-resources/logstash-input-plugins/archive/refs/heads/master.zip

raw.githubusercontent/2arunpmohan/logstash-input-plugins/master/heartbeat/heartbeat.conf

input { 
	heartbeat { 
	message => "ok"
	interval => 5
	type => "heartbeat"
	}
}
output{
	if [type] == "heatbeat" {
	elasticsearch { 
	hosts => "http://localhost:9200"
	index => "heartbeat"
	}
}
stdout {codec => "rubydebug"}
}


#sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/logstash-input-plugins/heartbeat/heartbeat.conf
#curl -XGET "http://localhost:9200/heartbeat/_search?pretty=true" -d'{"size":1}'

#sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/logstash-input-plugins/heartbeat/heartbeat-epoch.conf
#curl -XGET "http://localhost:9200/heartbeat-epoch/_search?pretty=true" -d'{"size":1}'

#sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/logstash-input-plugins/heartbeat/heartbeat-sequence.conf
#curl -XGET "http://localhost:9200/heartbeat-sequence/_search?pretty=true" -d'{ "sort": [{ "@timestamp": {"order":"asc"} } ] }'

-----------------------------------------------
==> [61. "Logstash Input Plugins, Part-2: Generator Input and Dead Letter"]
